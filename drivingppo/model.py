from typing import Callable, Literal
import os, time, random
from collections import defaultdict

from .world import World
from .environment import WorldEnv
from .common import (
    WORLD_DT,
    ACTION_REPEAT,
    LOOKAHEAD_POINTS,
    EACH_POINT_INFO_SIZE,
    LIDAR_NUM,
    OBSERVATION_IND_SPD,
    OBSERVATION_IND_WPOINT_0,
    OBSERVATION_IND_WPOINT_1,
    OBSERVATION_IND_WPOINT_E,
    OBSERVATION_DIM_WPOINT,
    OBSERVATION_IND_LIDAR_S,
    OBSERVATION_IND_LIDAR_E,
    OBSERVATION_DIM_LIDAR,
    OBSERVATION_DIM,
)

import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.vec_env import VecEnv, SubprocVecEnv, DummyVecEnv
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor


# í›ˆë ¨ ê²°ê³¼ ì €ì¥
LOG_DIR = f"./logs/"
CHECKPOINT_DIR = './checks/'


class NoFeaturesExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box):

        super(NoFeaturesExtractor, self).__init__(observation_space, features_dim=OBSERVATION_DIM)

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        return observations



class MLPFeaturesExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box):

        hidden_dim = 16
        flatten_output_dim = LOOKAHEAD_POINTS * hidden_dim

        super(MLPFeaturesExtractor, self).__init__(observation_space, features_dim=1 + flatten_output_dim)

        input_dim = LOOKAHEAD_POINTS * EACH_POINT_INFO_SIZE

        self.layer0 = nn.Sequential(
            nn.Flatten(),
            nn.Linear(input_dim, flatten_output_dim),
            nn.ReLU(),
            nn.Linear(flatten_output_dim, flatten_output_dim),
            nn.ReLU()
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        speed     = observations[:, OBSERVATION_IND_SPD      : OBSERVATION_IND_SPD+1]
        path_data = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_E]

        output0 = self.layer0(path_data)

        return torch.cat((speed, output0), dim=1)


class RNNFeaturesExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box):

        hidden_dim = 16
        flatten_output_dim = LOOKAHEAD_POINTS * hidden_dim

        super(RNNFeaturesExtractor, self).__init__(observation_space, features_dim=1 + flatten_output_dim)

        self.layer0 = nn.RNN(
            input_size=EACH_POINT_INFO_SIZE,
            hidden_size=hidden_dim,
            num_layers=1,
            batch_first=True,
            nonlinearity='relu'
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        speed     = observations[:, OBSERVATION_IND_SPD      : OBSERVATION_IND_SPD+1]
        path_data = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_E]

        reshaped_path = path_data.reshape(-1, LOOKAHEAD_POINTS, EACH_POINT_INFO_SIZE)  # [Batch, ê¸¸ì´Ã—ì±„ë„] -> [Batch, ê¸¸ì´(ì‹œê°„), ì±„ë„]
        output, _ = self.layer0(reshaped_path)

        output0 = torch.flatten(output, start_dim=1)

        return torch.cat((speed, output0), dim=1)


class CNNFeaturesExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box):

        hidden_dim = 16
        flatten_output_dim = LOOKAHEAD_POINTS * hidden_dim

        super(CNNFeaturesExtractor, self).__init__(observation_space, features_dim=1 + flatten_output_dim)

        self.cnn = nn.Sequential(
            nn.Conv1d(
                in_channels=EACH_POINT_INFO_SIZE,
                out_channels=hidden_dim,
                kernel_size=2,
                stride=1,
                padding=1
            ),
            nn.ReLU()
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        speed     = observations[:, OBSERVATION_IND_SPD      : OBSERVATION_IND_SPD+1]
        path_data = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_E]

        reshaped = path_data.reshape(-1, LOOKAHEAD_POINTS, EACH_POINT_INFO_SIZE).permute(0, 2, 1)  # [Batch, ê¸¸ì´Ã—ì±„ë„] -> [Batch, ì±„ë„, ê¸¸ì´]

        feature = self.cnn(reshaped)[:, :, :LOOKAHEAD_POINTS]

        output0 = torch.flatten(feature, start_dim=1)

        return torch.cat((speed, output0), dim=1)


class MyFeatureExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box):

        self.cascade_hidden_dim = 16
        feature1_dim = 64
        feature2_dim = 128
        total_feature_dim = 1 + OBSERVATION_DIM_WPOINT + feature1_dim + feature2_dim

        super(MyFeatureExtractor, self).__init__(observation_space, features_dim=total_feature_dim)

        # ë¼ì´ë‹¤ CNN
        self.layer1 = nn.Sequential(
            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=2, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv1d(in_channels=4, out_channels=6, kernel_size=6, stride=2, padding=0),
            nn.ReLU(),
            nn.AdaptiveMaxPool1d(output_size=feature1_dim),
            nn.Flatten(),
            nn.Linear(6 * feature1_dim, feature1_dim)
        )

        # ì†ë„ + ì²«ë²ˆì§¸ëª©í‘œ + ë¼ì´ë‹¤
        self.layer2 = nn.Sequential(
            nn.Linear(1 + EACH_POINT_INFO_SIZE + feature1_dim, feature2_dim),
            nn.ReLU(),
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        speed           = observations[:, OBSERVATION_IND_SPD      : OBSERVATION_IND_SPD+1]
        wpoint0         = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_1]
        path_data       = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_E]
        lidar_dis_data  = observations[:, OBSERVATION_IND_LIDAR_S  : OBSERVATION_IND_LIDAR_E]

        output1 = self.layer1(lidar_dis_data.unsqueeze(1))
        output2 = self.layer2(torch.cat((speed, wpoint0, output1), dim=1))

        return torch.cat((speed, path_data, output1, output2), dim=1)


class TensorboardCallback(BaseCallback):
    """
    í™˜ê²½(Env)ì—ì„œ info['episode_metrics']ë¡œ ì „ë‹¬í•œ ì»¤ìŠ¤í…€ ê°’ì„
    í…ì„œë³´ë“œì— ê¸°ë¡(Log)í•˜ëŠ” ì½œë°±
    """
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        # ë²¡í„° í™˜ê²½(VecEnv)ì„ ê³ ë ¤í•˜ì—¬ ëª¨ë“  í™˜ê²½ì˜ infoë¥¼ í™•ì¸
        for i, done in enumerate(self.locals['dones']):
            if done:
                info = self.locals['infos'][i]
                # WorldEnv.stepì—ì„œ ë„£ì–´ì¤€ episode_metricsê°€ ìˆìœ¼ë©´ ê¸°ë¡
                if 'episode_metrics' in info:
                    for key, value in info['episode_metrics'].items():
                        self.logger.record(key, value)
        return True


def train_start(
        gen_env:Callable[[], WorldEnv],
        steps:int=0,  # 0: í•™ìŠµ ì—†ì´ ìƒì„±ë§Œ ëœ ëª¨ë¸ ë°˜í™˜.
        save_path:str|None=None,
        save_freq:int=0,
        tb_log:bool=False,
        run_name:str='DPPO',
        *,
        vec_env:Literal['dummy', 'subp']|VecEnv='dummy',
        lr=3e-4,
        gamma=0.9,
        ent_coef=0.01,
        n_steps=1024,
        batch_size=256,
        progress_bar=True,
        seed=42
) -> PPO:
    """
    í•™ìŠµ ì²˜ìŒë¶€í„°
    """

    if vec_env == 'dummy':
        vec_env_cls = DummyVecEnv
    elif vec_env == 'subp':
        vec_env_cls = SubprocVecEnv
    else:
        raise Exception(f'unknown vec_env: {vec_env}')
    vec_env = make_vec_env(gen_env, n_envs=1, vec_env_cls=vec_env_cls, seed=seed)# n_envs: ë³‘ë ¬ í™˜ê²½ ìˆ˜

    policy_kwargs = dict(
        features_extractor_class=NoFeaturesExtractor,
        features_extractor_kwargs=dict(),
        net_arch=dict(
            pi=[512, 512], # Actor
            vf=[512, 512, 256]  # Critic
        )
    )
    print('POLICY:', policy_kwargs)

    # PPO ëª¨ë¸
    model = PPO(
        "MlpPolicy",
        vec_env,
        policy_kwargs=policy_kwargs,

        verbose=1,
        tensorboard_log=LOG_DIR  if tb_log  else None,

        # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        learning_rate=lr,
        gamma=gamma,           # ë¯¸ë˜ ë³´ìƒ í• ì¸ìœ¨
        ent_coef=ent_coef,     # ì—”íŠ¸ë¡œí”¼: ì¥ì• ë¬¼ ê±°ì˜ ì—†ëŠ” í™˜ê²½ - ì•½í•˜ê²Œ
        n_steps=n_steps,       # ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í… (ë²„í¼ í¬ê¸°, NUM_ENVS * n_steps = ì´ ìˆ˜ì§‘ ë°ì´í„°ëŸ‰)
        batch_size=batch_size, # ë¯¸ë‹ˆ ë°°ì¹˜ í¬ê¸°

        device="auto"  # GPU ì‚¬ìš© ì„¤ì •
    )

    if steps <= 0:
        # í•™ìŠµ ì—†ì´ ëª¨ë¸ ë°”ë¡œ ë°˜í™˜
        print("=== PPO ëª¨ë¸ ìƒì„± ===")
        if save_path:
            model.save(CHECKPOINT_DIR+save_path)
        return model

    # ì½œë°±
    callbacks:list[BaseCallback] = [TensorboardCallback()]  # ìš”ì†Œë³„ ì ìˆ˜
    if save_freq:
        # ëª¨ë¸ ì €ì¥ ì½œë°±
        checkpoint_callback = CheckpointCallback(
            save_freq=save_freq,
            save_path=CHECKPOINT_DIR,
            name_prefix='check'
        )
        callbacks.append(checkpoint_callback)

    print("=== PPO í•™ìŠµ ì‹œì‘ ===")

    model.learn(
        total_timesteps=steps,
        callback=callbacks,
        tb_log_name=run_name,
        log_interval=10,
        progress_bar=progress_bar,
    )

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    if save_path:
        model.save(CHECKPOINT_DIR+save_path)
        print(f"=== í•™ìŠµ ì™„ë£Œ: {CHECKPOINT_DIR+save_path} ===")

    vec_env.close()  # í™˜ê²½ ì •ë¦¬

    return model


def train_resume(
        model:PPO|str,
        gen_env: Callable[[], WorldEnv],
        steps: int,
        reset_num_timesteps=False,
        save_path:str|None=None,
        save_freq:int=0,
        tb_log:bool=False,
        run_name:str='DPPO',
        *,
        vec_env:Literal['dummy', 'subp']|VecEnv='dummy',
        log_std=None,
        lr=1e-4,
        gamma=0.99,
        ent_coef=0.0,
        progress_bar=True,
        seed=42
) -> PPO:
    """
    ê¸°ì¡´ ëª¨ë¸ ì¶”ê°€í•™ìŠµ
    """

    if vec_env == 'dummy':
        vec_env_cls = DummyVecEnv
    elif vec_env == 'subp':
        vec_env_cls = SubprocVecEnv
    else:
        raise Exception(f'unknown vec_env: {vec_env}')
    vec_env = make_vec_env(gen_env, n_envs=4, vec_env_cls=vec_env_cls, seed=seed)

    is_temp_file = False
    if isinstance(model, str):
        model_loading_path = model
    elif isinstance(model, PPO):
        model_loading_path = 'temp_model-' + time.strftime('%y%m%d%H%M%S') + str(random.randint(0, 9999))
        is_temp_file = True
        model.save(CHECKPOINT_DIR + model_loading_path)
    else:
        raise ValueError(f'model should be str or PPO not {type(model)}')

    print(f"=== ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {CHECKPOINT_DIR+model_loading_path} ===")
    model = PPO.load(
        path=CHECKPOINT_DIR+model_loading_path,
        env=vec_env,
        verbose=1,
        tensorboard_log=LOG_DIR  if tb_log  else None,

        learning_rate=lr,
        gamma=gamma,
        ent_coef=ent_coef,
    )
    assert isinstance(model, PPO)

    # ì½œë°±
    callbacks:list[BaseCallback] = [TensorboardCallback()]  # ì»¤ìŠ¤í…€ ë§¤íŠ¸ë¦­
    if save_freq:
        # ëª¨ë¸ ì €ì¥ ì½œë°±
        checkpoint_callback = CheckpointCallback(
            save_freq=save_freq,
            save_path=CHECKPOINT_DIR,
            name_prefix='check'
        )
        callbacks.append(checkpoint_callback)

    if log_std:
        with torch.no_grad():
            model.policy.log_std.fill_(log_std)

    print(f"=== í•™ìŠµ ì¬ê°œ (í˜„ì¬ ìŠ¤í…: {model.num_timesteps} / ëª©í‘œ: {steps + model.num_timesteps} / ë‚¨ì€: {steps}) ===")

    model.learn(
        total_timesteps=steps,
        callback=callbacks,
        tb_log_name=run_name,
        log_interval=10,
        progress_bar=progress_bar,

        reset_num_timesteps=reset_num_timesteps # ë‚´ë¶€ íƒ€ì„ìŠ¤í… ì¹´ìš´í„° ì´ˆê¸°í™” ì—¬ë¶€
    )

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    if save_path:
        model.save(CHECKPOINT_DIR+save_path)
        print(f"=== í•™ìŠµ ì™„ë£Œ ({model.num_timesteps} ìŠ¤í…): {CHECKPOINT_DIR+save_path} ===")

    # ì„ì‹œ íŒŒì¼ ì‚­ì œ ë¡œì§
    if is_temp_file:
        temp_file_path = CHECKPOINT_DIR + model_loading_path + ".zip"
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

    vec_env.close()

    return model


def run(
        world_generator:Callable[[], World],
        model:PPO|str,
        time_spd=2.0,
        time_step=WORLD_DT,
        step_per_control=ACTION_REPEAT,
        auto_close_at_end=True,
    ):
    """
    ëª¨ë¸ ì‹œê°ì  í™•ì¸ìš© ì‹¤í–‰
    """

    env = WorldEnv(
        world_generator=world_generator,
        time_step=time_step,
        action_repeat=1,
        render_mode='debug',
        auto_close_at_end=auto_close_at_end
    )

    if type(model) == str:
        model = PPO.load(CHECKPOINT_DIR+model, env=env)
    assert isinstance(model, PPO)

    obs, info = env.reset()
    terminated = False
    truncated = False
    episode_reward = 0.0

    while not terminated and not truncated:

        action, _ = model.predict(obs, deterministic=True)  # ì—ì´ì „íŠ¸ê°€ í–‰ë™ ì„ íƒ
        for _ in range(step_per_control):
            obs, reward, terminated, truncated, info = env.step(action)  # í–‰ë™ ì‹¤í–‰
            episode_reward += reward
            env.render()  # ì‹œê°í™” í˜¸ì¶œ
            time.sleep(time_step / 1000.0 / time_spd)# ì‹œê°í™” í”„ë ˆì„ì„ ìœ„í•´ ë”œë ˆì´ ì¶”ê°€
            if terminated or truncated: break

    print(f"ì—í”¼ì†Œë“œ ì¢…ë£Œ. ì´ ë³´ìƒ: {episode_reward:.2f}")

    env.close()


def evaluate(
        model:PPO|str,
        world_generator:Callable[[], World],
        episode_num=100,
        verbose:bool=True,
        csv_path:str="",
) -> dict:
    import numpy as np
    import pandas as pd

    env = WorldEnv(
        world_generator=world_generator,
        render_mode=None,
    )

    if type(model) == str:
        model = PPO.load(CHECKPOINT_DIR+model, env=env)
    assert isinstance(model, PPO)

    print(f"{episode_num}íšŒ ì—í”¼ì†Œë“œ í‰ê°€ ì‹œì‘...")

    all_metrics = defaultdict(list)
    episode_rewards = []
    episode_lengths = []

    for i in range(episode_num):
        i1 = i+1
        checkPeriod = episode_num // 10
        obs, info = env.reset()
        done = False
        truncated = False
        total_reward = 0
        esteps = 0

        while not (done or truncated):
            # ì—í”¼ì†Œë“œ ì§„í–‰
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env.step(action)
            total_reward += reward
            esteps += 1

            if done or truncated:
                episode_rewards.append(total_reward)
                episode_lengths.append(esteps)

                if 'episode_metrics' in info:
                    for key, value in info['episode_metrics'].items():#type:ignore
                        all_metrics[key].append(value)

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if i1 % checkPeriod == 0:
                    if verbose: print(f"[{i1}/{episode_num}] ì™„ë£Œ - Reward: {total_reward:.2f}, Steps: {esteps}")

    if verbose: 
        print("\n" + "="*41)
        print(f"í‰ê°€ ê²°ê³¼ ({episode_num} ì—í”¼ì†Œë“œ í‰ê· )")
        print("="*41)
        print(f"Total Reward  : {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
        print(f"Episode Length: {np.mean(episode_lengths):.1f}")

    if all_metrics:
        df_metrics = pd.DataFrame(all_metrics)

        # ìˆ˜: í‰ê· , í‘œì¤€í¸ì°¨
        num_df = df_metrics.select_dtypes(include=[np.number])
        if not num_df.empty and verbose:
            print("-" * 41)
            summary = num_df.describe().loc[['mean', 'std']].T
            summary['mean'] = summary['mean'].map('{:.4f}'.format)  # ì§€ìˆ˜í‘œê¸° ì•ˆ í•¨.
            print(summary)

        # ë¬¸ìì—´(ë²”ì£¼í˜•): ì¢…ë¥˜ë³„ ë¹„ìœ¨
        cat_df = df_metrics.select_dtypes(exclude=[np.number])
        if not cat_df.empty and verbose:
            print("-" * 41)
            for col in cat_df.columns:
                print(f"\n* {col}")
                counts = df_metrics[col].value_counts()
                ratios = df_metrics[col].value_counts(normalize=True)

                for idx, val in counts.items():
                    ratio = ratios[idx] * 100  #type:ignore
                    print(f"   - {idx:<10}: {val:3d}íšŒ ({ratio:5.1f}%)")

        # CSV ì €ì¥
        if csv_path:
            df_metrics.to_csv(csv_path, index=False)
            if verbose: 
                print(f"\nğŸ’¾ ì„¸ë¶€ ê²°ê³¼ ì €ì¥ë¨: {csv_path}")
            if verbose: print(f"\nì„¸ë¶€ ê²°ê³¼ê°€ ì €ì¥: {csv_path}")
    else:
        if verbose: print("\nâš ï¸ info['episode_metrics']ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ.")

    env.close()

    return all_metrics
