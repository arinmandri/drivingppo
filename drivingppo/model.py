from typing import Callable, Literal
import os, time, random, json
from collections import defaultdict

from .world import World
from .environment import WorldEnv
from .common import (
    WORLD_DT,
    ACTION_REPEAT,
    LOOKAHEAD_POINTS,
    EACH_POINT_INFO_SIZE,
    LIDAR_NUM,
    OBSERVATION_IND_SPD,
    OBSERVATION_IND_WPOINT_0,
    OBSERVATION_IND_WPOINT_1,
    OBSERVATION_IND_WPOINT_E,
    OBSERVATION_DIM_WPOINT,
    OBSERVATION_IND_LIDAR_S,
    OBSERVATION_IND_LIDAR_E,
    OBSERVATION_DIM_LIDAR,
    OBSERVATION_DIM,
)

import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.vec_env import VecEnv, SubprocVecEnv, DummyVecEnv
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor


# í›ˆë ¨ ê²°ê³¼ ì €ì¥
LOG_DIR = f"./logs/"
CHECKPOINT_DIR = './checks/'


class NoFeaturesExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box):

        super(NoFeaturesExtractor, self).__init__(observation_space, features_dim=OBSERVATION_DIM)

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        return observations


class VMLPFeaturesExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box, output_dim=256):
        super(VMLPFeaturesExtractor, self).__init__(observation_space, features_dim=output_dim)

        self.layer = nn.Sequential(
            nn.Linear(OBSERVATION_DIM, output_dim),
            nn.ReLU(),
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        output = self.layer(observations)
        return output


class Shwp1FeaturesExtractor(BaseFeaturesExtractor):

    def __init__(self, observation_space: gym.spaces.Box, conv_out_channels=8):

        total_feature_dim = 1 + LOOKAHEAD_POINTS * conv_out_channels

        super().__init__(observation_space, features_dim=total_feature_dim)

        self.layer1 = nn.Sequential(
            nn.Conv1d(
                in_channels=1,
                out_channels=conv_out_channels,
                kernel_size=EACH_POINT_INFO_SIZE,
                stride=EACH_POINT_INFO_SIZE,
                padding=0),
            nn.ReLU(),
            nn.Flatten(),
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        speed           = observations[:, OBSERVATION_IND_SPD      : OBSERVATION_IND_SPD+1]
        path_data       = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_E]

        path_output = self.layer1(path_data.unsqueeze(1))

        return torch.cat((speed, path_output), dim=1)


class Shwp0FeaturesExtractor(BaseFeaturesExtractor):

    def __init__(self, observation_space: gym.spaces.Box, conv_out_channels=16):

        total_feature_dim = 1 + LOOKAHEAD_POINTS * conv_out_channels

        super().__init__(observation_space, features_dim=total_feature_dim)

        self.layer1 = nn.Sequential(
            nn.Conv1d(
                in_channels=1,
                out_channels=conv_out_channels,
                kernel_size=EACH_POINT_INFO_SIZE * 2,
                stride=EACH_POINT_INFO_SIZE,
                padding=0),
            nn.ReLU(),
            nn.Flatten(),
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        batch_size = observations.shape[0]

        speed           = observations[:, OBSERVATION_IND_SPD      : OBSERVATION_IND_SPD+1]
        path_data       = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_E]
        path_data_0     = observations.new_tensor([[0.0, 1.0, 1.0, 0.0]]).expand(batch_size, -1)  # ì²«ë²ˆì§¸ ì›¨ì´í¬ì¸íŠ¸ì™€ ê²°í•©ë  ì˜ì (ì—ì´ì „íŠ¸ì¤‘ì‹¬ì—ì„œ í‘œí˜„ëœ ì—ì´ì „íŠ¸)

        path_input_data = torch.cat((path_data_0, path_data), dim=1)
        path_output = self.layer1(path_input_data.unsqueeze(1))

        return torch.cat((speed, path_output), dim=1)


class MyFeatureExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Box):

        feature1_dim = 64
        feature2_dim = 128
        total_feature_dim = 1 + OBSERVATION_DIM_WPOINT + feature1_dim + feature2_dim

        super(MyFeatureExtractor, self).__init__(observation_space, features_dim=total_feature_dim)

        # ë¼ì´ë‹¤ CNN
        self.layer1 = nn.Sequential(
            nn.Conv1d(in_channels=1, out_channels=4, kernel_size=2, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv1d(in_channels=4, out_channels=6, kernel_size=6, stride=2, padding=0),
            nn.ReLU(),
            nn.AdaptiveMaxPool1d(output_size=feature1_dim),
            nn.Flatten(),
            nn.Linear(6 * feature1_dim, feature1_dim)
        )

        # ì†ë„ + ì²«ë²ˆì§¸ëª©í‘œ + ë¼ì´ë‹¤
        self.layer2 = nn.Sequential(
            nn.Linear(1 + EACH_POINT_INFO_SIZE + feature1_dim, feature2_dim),
            nn.ReLU(),
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        speed           = observations[:, OBSERVATION_IND_SPD      : OBSERVATION_IND_SPD+1]
        wpoint0         = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_1]
        path_data       = observations[:, OBSERVATION_IND_WPOINT_0 : OBSERVATION_IND_WPOINT_E]
        lidar_dis_data  = observations[:, OBSERVATION_IND_LIDAR_S  : OBSERVATION_IND_LIDAR_E]

        output1 = self.layer1(lidar_dis_data.unsqueeze(1))
        output2 = self.layer2(torch.cat((speed, wpoint0, output1), dim=1))

        return torch.cat((speed, path_data, output1, output2), dim=1)


class TensorboardCallback(BaseCallback):
    """
    í™˜ê²½(Env)ì—ì„œ info['episode_metrics']ë¡œ ì „ë‹¬í•œ ì»¤ìŠ¤í…€ ê°’ì„
    í…ì„œë³´ë“œì— ê¸°ë¡(Log)í•˜ëŠ” ì½œë°±
    """
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        # ë²¡í„° í™˜ê²½(VecEnv)ì„ ê³ ë ¤í•˜ì—¬ ëª¨ë“  í™˜ê²½ì˜ infoë¥¼ í™•ì¸
        for i, done in enumerate(self.locals['dones']):
            if done:
                info = self.locals['infos'][i]
                # WorldEnv.stepì—ì„œ ë„£ì–´ì¤€ episode_metricsê°€ ìˆìœ¼ë©´ ê¸°ë¡
                if 'episode_metrics' in info:
                    for key, value in info['episode_metrics'].items():
                        self.logger.record(key, value)
        return True


def linear_schedule(start:float, end:float=0.0) -> Callable[[float], float]:
    """
    í•™ìŠµ ì§„í–‰ë„ì— ë”°ë¼ í•™ìŠµë¥ ì„ ì„ í˜•ìœ¼ë¡œ ê°ì†Œì‹œí‚¤ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬.
    """
    def func(progress_remaining: float) -> float:
        return (progress_remaining * (start - end)) + end
    return func


def create_model(
        policy_kwargs=dict(
            features_extractor_class=NoFeaturesExtractor,
            features_extractor_kwargs=dict(),
            net_arch=dict(
                pi=[512, 512], # Actor
                vf=[512, 512, 256]  # Critic
            )
        ),
        save_path:str|None=None,
        *,
        n_steps=512,
        batch_size=256,
) -> PPO:
    """
    ëª¨ë¸ ìƒì„±ë§Œ
    """
    vec_env = make_vec_env(WorldEnv, n_envs=1, vec_env_cls=DummyVecEnv)# n_envs: ë³‘ë ¬ í™˜ê²½ ìˆ˜

    print('POLICY:', policy_kwargs)

    # PPO ëª¨ë¸
    model = PPO(
        "MlpPolicy",
        vec_env,
        policy_kwargs=policy_kwargs,
        n_steps=n_steps,       # ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í… (ë²„í¼ í¬ê¸°, NUM_ENVS * n_steps = ì´ ìˆ˜ì§‘ ë°ì´í„°ëŸ‰)
        batch_size=batch_size, # ë¯¸ë‹ˆ ë°°ì¹˜ í¬ê¸°
    )

    # ëª¨ë¸ ì €ì¥
    if save_path:
        model.save(CHECKPOINT_DIR+save_path)
        print(f"ëª¨ë¸ ì €ì¥: {CHECKPOINT_DIR+save_path}")

    vec_env.close()  # í™˜ê²½ ì •ë¦¬

    return model


def train(
        model:PPO|str,
        gen_env: Callable[[], WorldEnv],
        steps: int,
        reset_num_timesteps=False,
        save_path:str|None=None,
        save_freq:int=0,
        tb_log:bool=False,
        run_name:str='DPPO',
        *,
        vec_env:Literal['dummy', 'subp']|VecEnv='dummy',
        log_std=None,
        lr:float|Callable[[float], float]=1e-4,
        gamma=0.99,
        ent_coef=0.0,
        progress_display:Literal['tqdm', 'simple']|None='simple',
        seed=42
) -> PPO:
    """
    ê¸°ì¡´ ëª¨ë¸ ì¶”ê°€í•™ìŠµ
    """

    if vec_env == 'dummy':
        vec_env_cls = DummyVecEnv
    elif vec_env == 'subp':
        vec_env_cls = SubprocVecEnv
    else:
        raise Exception(f'unknown vec_env: {vec_env}')
    vec_env = make_vec_env(gen_env, n_envs=4, vec_env_cls=vec_env_cls, seed=seed)

    is_temp_file = False
    if isinstance(model, str):
        model_loading_path = model
    elif isinstance(model, PPO):
        model_loading_path = 'temp_model-' + time.strftime('%y%m%d%H%M%S') + str(random.randint(0, 9999))
        is_temp_file = True
        model.save(CHECKPOINT_DIR + model_loading_path)
    else:
        raise ValueError(f'model should be str or PPO not {type(model)}')

    print(f"=== ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {CHECKPOINT_DIR+model_loading_path} ===")
    model = PPO.load(
        path=CHECKPOINT_DIR+model_loading_path,
        env=vec_env,
        verbose=0,
        tensorboard_log=LOG_DIR  if tb_log  else None,

        learning_rate=lr,
        gamma=gamma,
        ent_coef=ent_coef,
    )
    assert isinstance(model, PPO)

    # ì½œë°±
    callbacks:list[BaseCallback] = []
    if tb_log:
        callbacks.append(TensorboardCallback())  # ì»¤ìŠ¤í…€ ë§¤íŠ¸ë¦­
    if progress_display == 'simple':
        callbacks.append(PercentageProgressCallback(total_timesteps=steps))
    if save_freq:
        # ëª¨ë¸ ì €ì¥ ì½œë°±
        checkpoint_callback = CheckpointCallback(
            save_freq=save_freq,
            save_path=CHECKPOINT_DIR,
            name_prefix='check'
        )
        callbacks.append(checkpoint_callback)

    if log_std:
        with torch.no_grad():
            model.policy.log_std.fill_(log_std)

    print(f"=== í•™ìŠµ (í˜„ì¬ ìŠ¤í…: {model.num_timesteps} | ëª©í‘œ: {steps + model.num_timesteps} | ë‚¨ì€: {steps}) ===")

    if steps > 0:
        model.learn(
            total_timesteps=steps,
            callback=callbacks,
            tb_log_name=run_name,
            log_interval=10,
            progress_bar=True  if progress_display == 'tqdm'  else False,

            reset_num_timesteps=reset_num_timesteps # ë‚´ë¶€ íƒ€ì„ìŠ¤í… ì¹´ìš´í„° ì´ˆê¸°í™” ì—¬ë¶€
        )

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    if save_path:
        model.save(CHECKPOINT_DIR+save_path)
        print(f"=== í•™ìŠµ ì™„ë£Œ ({model.num_timesteps} ìŠ¤í…): {CHECKPOINT_DIR+save_path} ===")

    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    if is_temp_file:
        temp_file_path = CHECKPOINT_DIR + model_loading_path + ".zip"
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

    vec_env.close()

    return model


def run(
        world_generator:Callable[[], World]|World,
        model:PPO|str,
        time_spd=2.0,
        time_step=WORLD_DT,
        action_repeat=ACTION_REPEAT,
        briefly=True,
        auto_close_at_end=True,
    ):
    """
    ëª¨ë¸ ì‹œê°ì  í™•ì¸ìš© ì‹¤í–‰
    """
    if isinstance(world_generator, Callable):
        wgen = world_generator
    elif isinstance(world_generator, World):
        wgen = lambda: world_generator
    else:
        raise ValueError()

    env = WorldEnv(
        world_generator=wgen,
        time_step=time_step,
        action_repeat=action_repeat  if briefly  else 1,
        render_mode='debug',
        auto_close_at_end=auto_close_at_end
    )

    if type(model) == str:
        model = PPO.load(CHECKPOINT_DIR+model, env=env)
    assert isinstance(model, PPO)

    obs, info = env.reset()
    terminated = False
    truncated = False
    episode_reward = 0.0

    while not terminated and not truncated:

        action, _ = model.predict(obs, deterministic=True)  # ì—ì´ì „íŠ¸ê°€ í–‰ë™ ì„ íƒ
        for _ in range(1  if briefly  else action_repeat):
            obs, reward, terminated, truncated, info = env.step(action)  # í–‰ë™ ì‹¤í–‰
            episode_reward += reward
            env.render()  # ì‹œê°í™” í˜¸ì¶œ
            time.sleep(time_step * action_repeat / 1000.0 / time_spd)# ì‹œê°í™” í”„ë ˆì„ì„ ìœ„í•´ ë”œë ˆì´ ì¶”ê°€
            if terminated or truncated: break

    print(json.dumps(info, indent=4))
    print(f"ì—í”¼ì†Œë“œ ì¢…ë£Œ. ì´ ë³´ìƒ: {episode_reward:.2f}")

    env.close()


def evaluate(
        model:PPO|str,
        world_generator:Callable[[], World],
        episode_num=100,
        csv_path:str="",
        *,
        time_step=WORLD_DT,
        action_repeat=ACTION_REPEAT,
        print_result:bool=True,
        verbose:bool=True,
) -> dict:
    import numpy as np
    import pandas as pd

    env = WorldEnv(
        world_generator=world_generator,
        time_step=time_step,
        action_repeat=action_repeat,
        render_mode=None,
    )

    if type(model) == str:
        model = PPO.load(CHECKPOINT_DIR+model, env=env)
    assert isinstance(model, PPO)

    if verbose: print(f"{episode_num}íšŒ ì—í”¼ì†Œë“œ í‰ê°€ ì‹œì‘...")

    all_metrics = defaultdict(list)
    episode_rewards = []

    for i in range(episode_num):
        i1 = i+1
        checkPeriod = episode_num // 10
        obs, info = env.reset()
        done = False
        truncated = False
        total_reward = 0

        while not (done or truncated):
            # ì—í”¼ì†Œë“œ ì§„í–‰
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env.step(action)
            total_reward += reward

            if done or truncated:
                episode_rewards.append(total_reward)

                if 'episode_metrics' in info:
                    for key, value in info['episode_metrics'].items():#type:ignore
                        all_metrics[key].append(value)

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if i1 % checkPeriod == 0:
                    if verbose: print(f"[{i1}/{episode_num}] ì™„ë£Œ - Reward: {total_reward:.2f}")

    # í‰ê°€ ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    if print_result:
        print("="*41)
        print(f"í‰ê°€ ê²°ê³¼ ({episode_num} ì—í”¼ì†Œë“œ í‰ê· )")
        print("-"*41)
        print(f"Total Reward  : {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")

    if all_metrics:
        df_metrics = pd.DataFrame(all_metrics)

        # ìˆ˜: í‰ê· , í‘œì¤€í¸ì°¨
        num_df = df_metrics.select_dtypes(include=[np.number])
        if not num_df.empty and print_result:
            print("-" * 41)
            summary = num_df.describe().loc[['mean', 'std']].T
            summary['mean'] = summary['mean'].map('{:.4f}'.format)  # ì§€ìˆ˜í‘œê¸° ì•ˆ í•¨.
            print(summary)

        # ë¬¸ìì—´(ë²”ì£¼í˜•): ì¢…ë¥˜ë³„ ë¹„ìœ¨
        cat_df = df_metrics.select_dtypes(exclude=[np.number])
        if not cat_df.empty and print_result:
            print("-" * 41)
            for col in cat_df.columns:
                print(f"* {col}")
                counts = df_metrics[col].value_counts()
                ratios = df_metrics[col].value_counts(normalize=True)

                for idx, val in counts.items():
                    ratio = ratios[idx] * 100  #type:ignore
                    print(f"   - {idx:<10}: {val:3d}íšŒ ({ratio:5.1f}%)")
        if print_result:
            print("="*41)

        # CSV ì €ì¥
        if csv_path:
            df_metrics.to_csv(csv_path, index=False)
            if verbose:
                print(f"\nğŸ’¾ ì„¸ë¶€ ê²°ê³¼ ì €ì¥ë¨: {csv_path}")
            if verbose: print(f"\nì„¸ë¶€ ê²°ê³¼ê°€ ì €ì¥: {csv_path}")
    else:
        if verbose: print("\nâš ï¸ info['episode_metrics']ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ.")

    env.close()

    return all_metrics

class PercentageProgressCallback(BaseCallback):
    """ì‹¤ì œ ëˆ„ì  íƒ€ì„ìŠ¤í…(num_timesteps)ì„ ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ í•™ìŠµì˜ 10% ë‹¨ìœ„ ì§„í–‰ë„ë¥¼ ì¶œë ¥í•˜ëŠ” ì½œë°±"""
    def __init__(self, total_timesteps: int, verbose=0):
        super().__init__(verbose)
        self.total_timesteps = total_timesteps
        self.start_time = 0
        self.next_target_percent = 10  # ìµœì´ˆ ëª©í‘œ
        self.initial_num_timesteps = 0 # ì´ë²ˆ í•™ìŠµ ì„¸ì…˜ì˜ ì‹œì‘ ìŠ¤í…

        # ë‚¨ì€ ì‹œê°„ ê³„ì‚°ì„ ìœ„í•œ ì§ì „ ë§ˆì¼ìŠ¤í†¤(10% ë‹¨ìœ„)ì˜ ì‹œê°„ê³¼ ìŠ¤í… ê¸°ë¡
        self.last_milestone_time = 0
        self.last_milestone_steps = 0

    def _on_training_start(self) -> None:
        self.start_time = time.time()
        self.last_milestone_time = self.start_time

        # í•™ìŠµ ì‹œì‘ ì‹œì ì˜ ëª¨ë¸ ëˆ„ì  ìŠ¤í…ì„ ê¸°ì¤€ì ìœ¼ë¡œ ì €ì¥
        self.initial_num_timesteps = self.model.num_timesteps
        self.last_milestone_steps = 0

    def _on_step(self) -> bool:
        # ì´ë²ˆ learn() í˜¸ì¶œì—ì„œ ìˆœìˆ˜í•˜ê²Œ ì§„í–‰ëœ ìŠ¤í… ê³„ì‚°
        current_progress = self.num_timesteps - self.initial_num_timesteps

        # ëª©í‘œ ìŠ¤í… ê³„ì‚°
        target_step = int(self.total_timesteps * (self.next_target_percent / 100.0))

        # ìˆœìˆ˜ ì§„í–‰ ìŠ¤í…ì´ ëª©í‘œ ìŠ¤í… ì´ìƒ ë„ë‹¬ ì‹œ ì¶œë ¥
        if current_progress >= target_step:
            current_time = time.time()
            elapsed = current_time - self.start_time
            remaining_steps = max(0, self.total_timesteps - current_progress)

            m, s = divmod(int(elapsed), 60)
            h, m = divmod(m, 60)

            eta_str = ""
            # 100% ì´í•˜ì¼ ë•Œë§Œ ì§ì „ êµ¬ê°„ì˜ ì†ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‚¨ì€ ì‹œê°„ ê³„ì‚°
            if self.next_target_percent < 100:
                delta_time = current_time - self.last_milestone_time
                delta_steps = current_progress - self.last_milestone_steps
                
                if delta_steps > 0:
                    time_per_step = delta_time / delta_steps
                    eta_seconds = int(time_per_step * remaining_steps)

                    eta_m, eta_s = divmod(eta_seconds, 60)
                    eta_h, eta_m = divmod(eta_m, 60)
                    eta_str = f" | ë‚¨ì€ ì‹œê°„: {eta_h:02d}:{eta_m:02d}:{eta_s:02d}"

            print(f"[ì§„í–‰ë„ {self.next_target_percent:3d}%] ê²½ê³¼ ì‹œê°„: {h:02d}:{m:02d}:{s:02d}{eta_str} | "
                  f"ì´ë²ˆ ëª©í‘œ: {current_progress} / {self.total_timesteps} ìŠ¤í… | "
                  f"(ëª¨ë¸ ì´ ëˆ„ì : {self.num_timesteps})")

            # ë‹¤ìŒ êµ¬ê°„ ê³„ì‚°ì„ ìœ„í•´ í˜„ì¬ ìƒíƒœë¥¼ ë§ˆì¼ìŠ¤í†¤ìœ¼ë¡œ ê°±ì‹ 
            self.last_milestone_time = current_time
            self.last_milestone_steps = current_progress
            self.next_target_percent += 10

        return True
