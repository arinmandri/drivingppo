"""
PPO ëª¨ë¸ ì„¤ì •ê°’ë“¤, í›ˆë ¨ í™˜ê²½ê³¼ í›ˆë ¨ í•¨ìˆ˜ë“¤, ê°ì¢… ë³€í™˜ í•¨ìˆ˜, ìœ í‹¸ ë“±.
"""
from typing import Callable, Literal
import math
from datetime import datetime

from .world import World, distance_of, angle_of, pi, pi2, rad_to_deg
from .simsim import WorldViewer
from .common import (
    SPD_MAX_STD,
    LOOKAHEAD_POINTS,
    OBSERVATION_IND_SPD,
    OBSERVATION_IND_WPOINT_0,
    OBSERVATION_IND_WPOINT_1,
    OBSERVATION_IND_WPOINT_2,
    OBSERVATION_DIM,
)

import numpy as np
from numpy import ndarray as Arr
import gymnasium as gym
from gymnasium import spaces


def get_state(world:World):
    """
    Worldì˜ í˜„ì¬ ìƒíƒœë¥¼ RL ì…ë ¥ ë²¡í„°(ê³ ì • í¬ê¸°)ë¡œ ë³€í™˜
    """
    p = world.player
    s_norm = speed_norm(p.speed)

    # ê²½ë¡œ ì •ë³´
    path_data = get_path_features(world)

    # ëª¨ë“  ë²¡í„°ë¥¼ í•©ì³ ê³ ì •ëœ í¬ê¸°ì˜ ë°°ì—´ë¡œ ë§Œë“ ë‹¤.
    observation = np.array([s_norm] + path_data, dtype=np.float32)

    return observation

def speed_norm(speed):
    return speed / SPD_MAX_STD

def get_path_features(world:World) -> list[float]:
    """
    ê²½ë¡œ ì •ë³´
    ë°”ë¡œ ì•ì˜ ì  ëª‡ ê°œì˜ ê±°ë¦¬ì™€ ê°ë„.
    """

    path_data = []
    x0 = world.player.x
    z0 = world.player.z
    a0 = world.player.angle_x

    # ê° ëª©í‘œì ì˜ ê±°ë¦¬, ê°ë„ ì •ë³´
    for index in range(
            world.waypoint_idx,
            world.waypoint_idx + LOOKAHEAD_POINTS
        ):
        # ì´ì „ ëª©í‘œì  ê¸°ì¤€
        if index < world.path_len:
            x1, z1 = world.waypoints[index]
            d_from_prev = distance_of(x0, z0, x1, z1)
            a1          = angle_of(x0, z0, x1, z1)
            a_from_prev = a1 - a0
            x0 = x1
            z0 = z1
            a0 = a1
        else:
            d_from_prev = 0.0
            a_from_prev = 0.0

        a_fp_norm = ((a_from_prev + pi) % pi2 - pi) / pi  # ê°ë„(ì´ì „ ëª©í‘œì  ê¸°ì¤€)
        d_near = distance_score_near(d_from_prev)  # ê±°ë¦¬ ê°€ê¹Œìš´ ì •ë„
        d_far  = distance_score_far(d_from_prev)   # ê±°ë¦¬ ë¨¼ ì •ë„

        path_data.extend([a_fp_norm, math.cos(a_fp_norm), d_near, d_far])

    # # ê° ëª©í‘œì ì˜ ê±°ë¦¬, ê°ë„ ì •ë³´
    # for _ in range(LOOKAHEAD_POINTS):

    #     # ì—ì´ì „íŠ¸ ê¸°ì¤€
    #     d_from_agnt = world.get_distance_to_wpoint()
    #     a_from_agnt = world.get_relative_angle_to_wpoint()

    #     a_fp_norm = ((a_from_agnt + pi) % pi2 - pi) / pi  # ê°ë„(ì´ì „ ëª©í‘œì  ê¸°ì¤€)
    #     d_near = distance_score_near(d_from_agnt)  # ê±°ë¦¬ ê°€ê¹Œìš´ ì •ë„
    #     d_far  = distance_score_far(d_from_agnt)   # ê±°ë¦¬ ë¨¼ ì •ë„

    #     path_data.extend([a_fp_norm, math.cos(a_fp_norm), d_near, d_far])

    return path_data

def observation_str(observation):
    agent_speed       = observation[OBSERVATION_IND_SPD]
    obs_wpoint_afp_0  = observation[OBSERVATION_IND_WPOINT_0]
    obs_wpoint_dist_0 = observation[OBSERVATION_IND_WPOINT_0 +3]
    obs_wpoint_afp_1  = observation[OBSERVATION_IND_WPOINT_1]
    obs_wpoint_dist_1 = observation[OBSERVATION_IND_WPOINT_1 +3]
    obs_wpoint_afp_2  = observation[OBSERVATION_IND_WPOINT_2]
    obs_wpoint_dist_2 = observation[OBSERVATION_IND_WPOINT_2 +3]
    return f'STATE:  speed {agent_speed:+.2f}({speed_norm(agent_speed):+.2f})'\
           f' | Path'\
           f' [0] a:{obs_wpoint_afp_0*pi*rad_to_deg:+5.2f} d:{obs_wpoint_dist_0:.2f}'\
           f' [1] a:{obs_wpoint_afp_1*pi*rad_to_deg:+5.2f} d:{obs_wpoint_dist_1:.2f}'\
           f' [2] a:{obs_wpoint_afp_2*pi*rad_to_deg:+5.2f} d:{obs_wpoint_dist_2:.2f}'

def _distance_score_near(x:float) -> float:
    d = x + 10.0
    x = 100./d/d
    if x <= 1:
        return x
    else:
        return 1.0

def distance_score_near(x:float) -> float:
    return _distance_score_near(x)

def distance_score_far(x:float) -> float:
    return x / 30.0


def apply_action(world:World, action:Arr):
    """
    í–‰ë™ ë²¡í„° [A_forward, A_steer]ë¥¼ Worldì˜ ì œì–´ í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì ìš©
    """
    ws, ad = action
    world.set_action(ws, ad, False)

def action_str(action):
    return f'ACTION: {action[0]:.2f}  {action[1]:.2f}'



class WorldEnv(gym.Env):
    """
    Worldì—ì„œ ì£¼í–‰ë²•ì„ ê°•í™”í•™ìŠµí•˜ê¸° ìœ„í•œ gym í™˜ê²½ í´ë˜ìŠ¤.
    """

    def __init__(self,
                 world_generator:Callable[[], World],
                 max_time=120_000,
                 time_step=111,
                 wstep_per_control=3,
                 time_gain_per_waypoint_rate=500,
                 time_gain_limit=20_000,
                 render_mode:Literal['window','debug']|None=None,
                 auto_close_at_end=True):

        super().__init__()
        self.closed = False

        self.time_step = time_step  # ì›”ë“œì˜ 1ìŠ¤í…ë‹¹ íë¥´ëŠ” ì‹œê°„(ì²œë¶„ì´ˆ)
        self.wstep_per_control = wstep_per_control  # ì¡°ì‘ê°’ ë³€ê²½ì€ ì›”ë“œì˜ nìŠ¤í…ë§ˆë‹¤ í•œ ë²ˆ. Tank Challengeì—ì„œë„ FPSëŠ” 30ì´ì–´ë„ API ìš”ì²­ì€ ìµœì†Œ 0.1ì´ˆë§ˆë‹¤ í•œ ë²ˆìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥í•˜ë‹¤.
        self.max_time = max_time  # ìµœëŒ€ ì—í”¼ì†Œë“œ ê¸¸ì´(ì²œë¶„ì´ˆ)
        self.time_gain_per_waypoint_rate = time_gain_per_waypoint_rate  # ë‹¤ìŒ ëª©í‘œì ê¹Œì§€ ê±°ë¦¬ 1ë‹¹ íšë“ ì‹œê°„(ì²œë¶„ì´ˆ)
        self.time_gain_limit = time_gain_limit  # ë‚¨ì€ ì œí•œì‹œê°„ ìµœëŒ€ëŸ‰(ì²œë¶„ì´ˆ)

        # Action: [A_forward, A_steer]
        self.action_space = spaces.Box(  # Forward, Steer
            low=np.array([-1.0, -1.0], dtype=np.float32),
            high=np.array([1.0, 1.0], dtype=np.float32),
            dtype=np.float32
        )

        # Observation Space ì •ì˜ (ê³ ì •ëœ í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(OBSERVATION_DIM,),
            dtype=np.float32
        )

        self.world_generator = world_generator

        """
        render_mode
        None: ì¡°ìš©íˆ
        'window': ì°½ ë„ì›€
        'debug': ì°½ + í„°ë¯¸ë„ì— í…ìŠ¤íŠ¸
        """
        self.render_mode = render_mode
        self.auto_close_at_end = auto_close_at_end
        self.viewer:WorldViewer|None = None
        print(f'WorldEnv render:{self.render_mode}')


    @property
    def observation(self):
        return get_state(self.world)

    @property
    def time_remaining(self):
        return self.time_limit - self.world.t_acc

    def step(self, action):
        """
        í–‰ë™ì„ ì‹¤í–‰í•˜ê³ , ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ë¥¼ ë°˜í™˜
        """
        observation0 = self.observation

        if self.closed:  # ì°½ ë‹«ì•„ì„œ ì¢…ë£Œ
            return observation0, 0, False, True, {}

        self.estep_count += 1
        if self.render_mode == 'debug':
            print(f'{self.estep_count} step -------------------------- ë‚¨ì€ì‹œê°„ {int((self.time_remaining)/1000)}')
            print(observation_str(observation0))

        self.action_history.append(action)  # ë§¤ìŠ¤í… ì•¡ì…˜ ê¸°ë¡
        ws, ad = action

        w = self.world
        p = w.player

        ang_pv  = w.get_relative_angle_to_wpoint()
        cos_pv  = math.cos(ang_pv)

        # ì•¡ì…˜ ì ìš©
        apply_action(self.world, action)
        result_collision = False
        result_wpoint = False
        for _ in range(self.wstep_per_control):
            _, result_collision_step, result_wpoint_step = w.step(self.time_step)
            result_collision += result_collision_step
            result_wpoint      += result_wpoint_step
        if self.estep_count == 1:
            if result_collision: print(f'ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ ë§µ í™•ì¸ í•„ìš”: ì‹œì‘ê³¼ë™ì‹œì— ì¶©ëŒ (hint: ëª©í‘œì  ìˆ˜ {w.path_len})')
            if result_wpoint:    print(f'ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ ë§µ í™•ì¸ í•„ìš”: ì‹œì‘ê³¼ë™ì‹œì— ê³¨ (hint: ëª©í‘œì  ìˆ˜ {w.path_len})')

        observation1 = self.observation

        if self.render_mode == 'debug': print(action_str(action))

        terminated = False
        truncated = False
        ending = ''

        s_norm = speed_norm(p.speed)  # ì†ë„ì ìˆ˜
        distance = w.get_distance_to_wpoint()
        ang_nx  = w.get_relative_angle_to_wpoint()
        cos_nx  = math.cos(ang_nx)

        self.speed_history.append(p.speed)

        reward_step = [0.0 for _ in range(7)]

        # ëª©í‘œì  ë„ë‹¬
        if result_wpoint:
            if p.speed > 0:  # í›„ì§„ ì§„í–‰ ì–µì œ
                reward_step[1] += 50.0

            # ì¶”ê°€ì‹œê°„ íšë“; ê·¸ëŸ¬ë‚˜ ë¬´í•œì • ìŒ“ì´ì§€ëŠ” ì•ŠìŒ.
            self.time_limit += int(distance * self.time_gain_per_waypoint_rate)
            self.time_limit = min(self.time_limit, w.t_acc + self.time_gain_limit)

            self.prev_d = self.prev_d1

            if self.render_mode == 'debug': print(f'â˜…[{w.waypoint_idx}] {reward_step[1]:.1f} ~ pass {int(round(ang_pv*rad_to_deg))}({cos_pv:.2f})')

            # ìµœì¢… ëª©í‘œ ë„ë‹¬
            if w.arrived:
                ending = 'arrived'
                terminated = True

        # ì „í˜€ ì—‰ëš±í•œ ê³³ ê°
        elif distance > w.far:
            reward_step[2] += 100.0 * p.speed / SPD_MAX_STD * cos_nx
            if self.render_mode == 'debug': print(f'LOST ({distance:.1f} > {w.far:.1f}) reward: {reward_step[2]:.2f}')
            ending = 'lost'
            truncated = True

        # ì‹œê°„ ë‚´ì— ë„ì°© ëª» í•¨
        elif w.t_acc >= self.time_limit:
            reward_step[2] += -150.0
            ending = 'timeover'
            truncated = True

        # íšë“í•œ ì‹œê°„ì€ ëª¨ìë¥´ì§€ ì•Šìœ¼ë‚˜ ê·¸ëƒ¥ ì´ì œê¹Œì§€ ë§ì´ í•¨.
        elif w.t_acc >= self.max_time:
            ending = 'timeout'
            truncated = True

        if truncated or terminated:
            icon = \
                'âœ…' if ending == 'arrived' else \
                'â–¶ï¸' if ending == 'timeout' else \
                'ğŸ‘»' if ending == 'lost' else \
                'â°' if ending == 'timeover' else '??'
            self.print_log(f'ê²°ê³¼{icon} ë„ì°©: {w.waypoint_idx:3d}/{w.path_len:3d} | ì‹œê°„: {int(w.t_acc/1000):3d}/{int(self.time_limit/1000):3d}/{int(self.max_time/1000):3d} ì´ˆ ({int(w.t_acc/self.max_time*100):3d}%) | ìœ„ì¹˜: {int(p.x):4d}, {int(p.z):4d} ({int(p.x/self.world.MAP_W*100):3d}%, {int(p.z/self.world.MAP_H*100):3d}%)')

        else:
            # ì§„í–‰ ë³´ìƒ

            reward_time = -0.1

            distance_d = distance - self.prev_d
            stat_progress     = - distance_d * 0.15  if s_norm > 0 \
                           else - self.wstep_per_control * s_norm * s_norm * 1.5  # í›„ì§„ ì§„í–‰ ì–µì œ
            reward_action_ws  = - ws**2 * 0.7
            reward_action_ad  = - ad**2 * 0.9
            total = reward_time + stat_progress + reward_action_ws + reward_action_ad
            if self.render_mode == 'debug': print(f'REWARD: time {reward_time:+5.2f} |  prog {stat_progress:+5.2f} | ws {reward_action_ws:+4.2f} | ad {reward_action_ad:+4.2f} --> {total:+6.2f}')

            reward_step[2] += self.wstep_per_control * reward_time
            reward_step[3] += stat_progress
            reward_step[5] += self.wstep_per_control * reward_action_ws
            reward_step[6] += self.wstep_per_control * reward_action_ad

        info = {'current_time': w.t_acc / 1000.0}

        # ì ìˆ˜ í•©
        reward_step[0] = sum(reward_step[1:])
        for i in range(7):
            self.reward_totals[i] += reward_step[i]

        if truncated or terminated:

            # ì•¡ì…˜ ë¶„ì‚°
            if len(self.action_history) > 0:
                action_arr = np.array(self.action_history)
                ws_var = np.var(action_arr[:, 0])
                ad_var = np.var(action_arr[:, 1])
            else:
                ws_var, ad_var = 0.0, 0.0

            if len(self.speed_history) > 0:
                speed_arr = np.array(self.speed_history)
                speed_var = np.var(speed_arr)
                speed_mean = sum(self.speed_history) / len(self.speed_history)
            else:
                speed_var = 0.0
                speed_mean = 0.0

            wstep_count = self.estep_count * self.wstep_per_control

            info['episode_metrics'] = {
                'ending/type': ending,
                'ending/estep': self.estep_count,
                'ending/wstep': self.estep_count * self.wstep_per_control,
                'rewards/0.total':       self.reward_totals[0]/wstep_count,
                'rewards/1.wPoint':      self.reward_totals[1]/wstep_count,
                'rewards/2.time':        self.reward_totals[2]/wstep_count,
                'rewards/3.progress':    self.reward_totals[3]/wstep_count,
                'rewards/5.ws':          self.reward_totals[5]/wstep_count,
                'rewards/6.ad':          self.reward_totals[6]/wstep_count,
                'metrics/ws_var':        ws_var,
                'metrics/ad_var':        ad_var,
                'metrics/speed_mean':    speed_mean,
                'metrics/speed_var':     speed_var,
            }

            self.print_result()

        self.prev_d = distance
        self.prev_d1 = w.get_distance_to_wpoint(1)

        # Gymnasium í‘œì¤€ ë°˜í™˜
        return observation1, reward_step[0], terminated, truncated, info


    def reset(self, *, seed=None, options=None):
        """
        í™˜ê²½ì„ ì´ˆê¸°í™”í•˜ê³  ì´ˆê¸° ìƒíƒœë¥¼ ë°˜í™˜
        """
        super().reset(seed=seed)

        w = self.world_generator()
        self.world = w

        self.estep_count = 0
        self.reward_totals = [0.0 for _ in range(7)]
        self.time_limit = self.time_gain_limit  # ì œí•œì‹œê°„. ëª©í‘œì  ë„ë‹¬ì‹œë§ˆë‹¤ ì¶”ê°€ íšë“.
        self.action_history = []  # ì•¡ì…˜ ê¸°ë¡
        self.speed_history = []

        self.prev_d  = w.get_distance_to_wpoint()
        self.prev_d1 = w.get_distance_to_wpoint(1)

        observation = self.observation
        info = {}
        return observation, info


    def render(self):
        if self.render_mode == None: return
        if self.closed:
            self.close()
            return

        # ì§€ì—° ì´ˆê¸°í™”: WorldViewerê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìƒì„±í•©ë‹ˆë‹¤.
        if self.viewer is None:
            self.viewer = WorldViewer(self.world, auto_update=False)
        elif self.viewer.world is not self.world:
            self.viewer.close()
            self.viewer = WorldViewer(self.world, auto_update=False)

        if self.viewer.closed: self.closed = True; return

        self.viewer.update()

    def print_result(self):
        wstep_count = self.estep_count * self.wstep_per_control
        if wstep_count:
            self.print_log(f'ì´ì  {int(self.reward_totals[0]):5d} '
                           f'| wpoint {self.reward_totals[1]:6.1f}({ int(self.reward_totals[1]/wstep_count*100)}%) '
                           f'| time {  self.reward_totals[2]:+7.2f}({int(self.reward_totals[2]/wstep_count*100)}%) '
                           f'| prog {  self.reward_totals[3]:+7.2f}({int(self.reward_totals[3]/wstep_count*100)}%) '
                           f'| ws {    self.reward_totals[5]:+7.2f}({int(self.reward_totals[5]/wstep_count*100)}%) '
                           f'| ad {    self.reward_totals[6]:+7.2f}({int(self.reward_totals[6]/wstep_count*100)}%)')

    def print_log(
            self,
            message: str,
    ):
        current_time = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
        formatted_message = f"{current_time} {message}"

        if self.render_mode == 'debug':
            print(formatted_message, flush=True)


    def close(self):
        self.print_result()
        self.closed = True
        if self.viewer is None: return
        if self.auto_close_at_end:
            self.viewer.close()
            self.viewer = None
        else:
            self.viewer.occupy_mainloop()
            self.viewer = None
        print('WorldEnv closed')
